{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Systems Final Project - Traffic Classification\n",
    "\n",
    "This notebook loads and processes network traffic data from a pcapng file with embedded pcapML labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (25.3)\n",
      "Requirement already satisfied: pcapml-fe in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (0.0.3)\n",
      "Requirement already satisfied: scapy in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pcapml-fe) (2.5.0)\n",
      "Requirement already satisfied: dpkt in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pcapml-fe) (1.9.8)\n",
      "Requirement already satisfied: pandas in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: autogluon in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: autogluon.core==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: autogluon.features==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon) (1.4.0)\n",
      "Requirement already satisfied: autogluon.tabular==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: autogluon.multimodal==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon) (1.4.0)\n",
      "Requirement already satisfied: autogluon.timeseries==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries[all]==1.4.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.0.2)\n",
      "Requirement already satisfied: scipy<1.17,>=1.5.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<1.8.0,>=1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.5.2)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.4.2)\n",
      "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.2.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.32.5)\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.10.7)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.42.4)\n",
      "Requirement already satisfied: autogluon.common==1.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (20.0.0)\n",
      "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (7.0.0)\n",
      "Requirement already satisfied: joblib<1.7,>=1.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.5.2)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\n",
      "Requirement already satisfied: ray<2.45,>=2.10.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.44.1)\n",
      "Requirement already satisfied: Pillow<12,>=10.0.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (11.3.0)\n",
      "Requirement already satisfied: torch<2.8,>=2.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.7.1)\n",
      "Requirement already satisfied: lightning<2.8,>=2.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.6.0)\n",
      "Requirement already satisfied: transformers<4.50,>=4.38.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (4.49.0)\n",
      "Requirement already satisfied: accelerate<2.0,>=0.34.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.12.0)\n",
      "Requirement already satisfied: fsspec<=2025.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2025.3.0)\n",
      "Requirement already satisfied: jsonschema<4.24,>=4.18 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (4.23.0)\n",
      "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.2.2)\n",
      "Requirement already satisfied: evaluate<0.5.0,>=0.4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.4.6)\n",
      "Requirement already satisfied: timm<1.0.7,>=0.9.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.0.3)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.16.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.22.1)\n",
      "Requirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\n",
      "Requirement already satisfied: torchmetrics<1.8,>=1.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.7.4)\n",
      "Requirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\n",
      "Requirement already satisfied: pytorch-metric-learning<2.9,>=1.3.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.8.1)\n",
      "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.1.11)\n",
      "Requirement already satisfied: nltk<3.10,>=3.4.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.2)\n",
      "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.9)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.6)\n",
      "Requirement already satisfied: tensorboard<3,>=2.9 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (2.20.0)\n",
      "Requirement already satisfied: pytesseract<0.4,>=0.3.9 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (0.3.13)\n",
      "Requirement already satisfied: nvidia-ml-py3<8.0,>=7.352.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (7.352.0)\n",
      "Requirement already satisfied: pdf2image<1.19,>=1.17.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon) (1.17.0)\n",
      "Requirement already satisfied: catboost<1.3,>=1.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (1.2.8)\n",
      "Requirement already satisfied: fastai<2.9,>=2.3.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.8.5)\n",
      "Requirement already satisfied: loguru in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.7.3)\n",
      "Requirement already satisfied: lightgbm<4.7,>=4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.6.0)\n",
      "Requirement already satisfied: einx in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.3.0)\n",
      "Requirement already satisfied: xgboost<3.1,>=2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.0.5)\n",
      "Requirement already satisfied: spacy<3.9 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.11)\n",
      "Requirement already satisfied: huggingface-hub[torch] in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.36.0)\n",
      "Requirement already satisfied: pytorch-lightning in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.6.0)\n",
      "Requirement already satisfied: gluonts<0.17,>=0.15.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.16.2)\n",
      "Requirement already satisfied: statsforecast<2.0.2,>=1.7.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.0.1)\n",
      "Requirement already satisfied: mlforecast<0.15.0,>=0.14.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.0)\n",
      "Requirement already satisfied: utilsforecast<0.2.12,>=0.2.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.2.11)\n",
      "Requirement already satisfied: coreforecast<0.0.17,>=0.0.12 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.0.16)\n",
      "Requirement already satisfied: fugue>=0.9.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.9.3)\n",
      "Requirement already satisfied: orjson~=3.9 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.11.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (0.7.0)\n",
      "Requirement already satisfied: botocore<1.43.0,>=1.42.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.42.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from botocore<1.43.0,>=1.42.4->boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from botocore<1.43.0,>=1.42.4->boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.5.0)\n",
      "Requirement already satisfied: graphviz in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\n",
      "Requirement already satisfied: plotly in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (6.5.0)\n",
      "Requirement already satisfied: six in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (1.17.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\n",
      "Requirement already satisfied: dill in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\n",
      "Requirement already satisfied: pip in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (25.3)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.9,>=1.8.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.8.17)\n",
      "Requirement already satisfied: fasttransform>=0.0.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\n",
      "Requirement already satisfied: plum-dispatch in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (2.6.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (3.1.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.13.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.12.5)\n",
      "Requirement already satisfied: toolz~=0.10 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.15.0)\n",
      "Requirement already satisfied: future in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\n",
      "Requirement already satisfied: py4j in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.29.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (80.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.2.5)\n",
      "Requirement already satisfied: numba in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.62.1)\n",
      "Requirement already satisfied: optuna in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.6.0)\n",
      "Requirement already satisfied: window-ops in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.0.15)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\n",
      "Requirement already satisfied: click in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (8.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (2025.11.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\n",
      "Requirement already satisfied: colorama in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.4.6)\n",
      "Requirement already satisfied: model-index in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.11)\n",
      "Requirement already satisfied: opendatalab in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.10)\n",
      "Requirement already satisfied: rich in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (14.2.0)\n",
      "Requirement already satisfied: tabulate in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.4.2)\n",
      "Requirement already satisfied: filelock in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.1.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (6.33.0)\n",
      "Requirement already satisfied: aiosignal in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: frozenlist in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.8.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.6.4)\n",
      "Requirement already satisfied: aiohttp-cors in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.8.1)\n",
      "Requirement already satisfied: colorful in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.5.8)\n",
      "Requirement already satisfied: opencensus in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.11.4)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.23.1)\n",
      "Requirement already satisfied: smart-open in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (7.5.0)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (20.35.4)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.76.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.10.5)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.5.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.20.0)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (2.3.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.1.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.14.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.21.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (1.2.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from weasel<0.5.0,>=0.4.2->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.23.0)\n",
      "Requirement already satisfied: wrapt in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from smart-open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.22.0)\n",
      "Requirement already satisfied: triad>=1.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.0.0)\n",
      "Requirement already satisfied: adagio>=0.2.6 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.2.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.14.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.45.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from sympy>=1.13.3->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.8)\n",
      "Requirement already satisfied: frozendict in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.7)\n",
      "Requirement already satisfied: ordered-set in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (4.1.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.72.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.43.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.6.1)\n",
      "Requirement already satisfied: pycryptodome in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (3.23.0)\n",
      "Requirement already satisfied: openxlab in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.0.11)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.0.44)\n",
      "Requirement already satisfied: Mako in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.3.10)\n",
      "Requirement already satisfied: tomli in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (2.13.0)\n",
      "Requirement already satisfied: beartype>=0.16.2 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from plum-dispatch->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.22.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (1.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# Install all required dependencies for pcapML → flows → features → ML\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Core pcapML tooling\n",
    "!pip install pcapml-fe\n",
    "\n",
    "# Data handling + ML\n",
    "!pip install pandas scikit-learn numpy\n",
    "\n",
    "# (Optional) AutoML — only if you want to compare to the leaderboard\n",
    "!pip install autogluon\n",
    "\n",
    "# Utility: progress bars (optional but recommended)\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed input to: /var/folders/94/0dl91t493jgdhwlhvd6vjdz00000gn/T/pcapml_fphpc15z/traffic.pcapng\n",
      "Iterating over pcapML samples and extracting features + labels...\n",
      "Feature frame shape: (158355, 49)\n",
      "Label frame shape:   (158355, 4)\n",
      "Merged frame shape: (158355, 52)\n",
      "Saved merged dataset to: /Users/connormcgraw/ml_systems_final/application_id_dataset.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "End-to-end script for pcapML Application Identification (non-vpn2016).\n",
    "\n",
    "Phase 1: Build a rich flow-level feature set from the pcapML dataset.\n",
    "\n",
    "Assumes:\n",
    "    - You are running in the same project directory that contains ./data/traffic.pcapng.gz\n",
    "    - You have installed: pcapml-fe, pandas, scikit-learn, numpy\n",
    "\n",
    "Input:\n",
    "    ./data/traffic.pcapng.gz\n",
    "\n",
    "Outputs:\n",
    "    ./application_id_dataset.csv   - features + labels\n",
    "    Prints Balanced Accuracy for a simple RandomForest on easy_label\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pcapml_fe   # pip install pcapml-fe\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Paths and decompression\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# For notebooks / scripts: use current working directory\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "INPUT_GZ = BASE_DIR / \"data\" / \"traffic.pcapng.gz\"\n",
    "\n",
    "assert INPUT_GZ.exists(), f\"Input file not found: {INPUT_GZ}\"\n",
    "\n",
    "# Decompress to a temporary .pcapng file (pcapml_fe wants plain pcapng)\n",
    "tmp_dir = tempfile.mkdtemp(prefix=\"pcapml_\")\n",
    "PCAP_PATH = Path(tmp_dir) / \"traffic.pcapng\"\n",
    "\n",
    "with gzip.open(INPUT_GZ, \"rb\") as f_in, open(PCAP_PATH, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())\n",
    "\n",
    "print(f\"Decompressed input to: {PCAP_PATH}\")\n",
    "print(\"Iterating over pcapML samples and extracting features + labels...\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Feature extraction from each pcapML sample\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _safe_moments(x: np.ndarray):\n",
    "    \"\"\"Return (mean, std, skew, kurt, cv) with safe handling for small arrays.\"\"\"\n",
    "    if x.size == 0:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    mean = float(x.mean())\n",
    "    std = float(x.std(ddof=0))\n",
    "    if std > 0:\n",
    "        z = (x - mean) / std\n",
    "        skew = float((z ** 3).mean())\n",
    "        kurt = float((z ** 4).mean())\n",
    "    else:\n",
    "        skew = 0.0\n",
    "        kurt = 0.0\n",
    "    cv = float(std / mean) if mean != 0 else 0.0\n",
    "    return mean, std, skew, kurt, cv\n",
    "\n",
    "\n",
    "def extract_features_from_sample(tsample):\n",
    "    \"\"\"\n",
    "    Given a pcapML traffic_sample, compute rich per-sample flow features.\n",
    "    Uses only packet sizes and timestamps, but tries to capture:\n",
    "      - size distribution\n",
    "      - timing / IAT distribution\n",
    "      - burstiness / temporal shape\n",
    "    Assumes:\n",
    "        tsample.packets: iterable of packet-like objects with:\n",
    "            - .raw_bytes (bytes)\n",
    "            - .ts (timestamp float)\n",
    "    \"\"\"\n",
    "    sizes = []\n",
    "    times = []\n",
    "\n",
    "    for pkt in tsample.packets:\n",
    "        # Defensive: some implementations might not have these attributes\n",
    "        try:\n",
    "            sizes.append(len(pkt.raw_bytes))\n",
    "            times.append(pkt.ts)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    if not sizes:\n",
    "        # zero-flow fallback: return zeros for all features\n",
    "        return {k: 0.0 for k in [\n",
    "            # basic\n",
    "            \"num_pkts\", \"total_bytes\", \"duration\",\n",
    "            # size stats\n",
    "            \"size_mean\", \"size_std\", \"size_skew\", \"size_kurt\", \"size_cv\",\n",
    "            \"size_min\", \"size_max\", \"size_median\",\n",
    "            \"size_q10\", \"size_q25\", \"size_q75\", \"size_q90\",\n",
    "            \"size_range\", \"size_iqr\", \"size_max_over_mean\",\n",
    "            # IAT stats\n",
    "            \"iat_mean\", \"iat_std\", \"iat_skew\", \"iat_kurt\", \"iat_cv\",\n",
    "            \"iat_min\", \"iat_max\", \"iat_median\",\n",
    "            \"iat_q10\", \"iat_q25\", \"iat_q75\", \"iat_q90\",\n",
    "            \"iat_range\", \"iat_iqr\", \"iat_max_over_mean\",\n",
    "            # rates\n",
    "            \"pkts_per_sec\", \"bytes_per_sec\",\n",
    "            # coarse size histogram (proportions)\n",
    "            \"prop_size_0_100\", \"prop_size_100_300\",\n",
    "            \"prop_size_300_600\", \"prop_size_600_1000\",\n",
    "            \"prop_size_gt_1000\",\n",
    "            # temporal / burstiness\n",
    "            \"prop_pkts_first_20\", \"prop_pkts_first_50\", \"prop_pkts_last_20\",\n",
    "            \"prop_bytes_first_20\", \"prop_bytes_first_50\", \"prop_bytes_last_20\",\n",
    "            \"time_fano_factor\", \"bytes_fano_factor\",\n",
    "        ]}\n",
    "\n",
    "    sizes = np.asarray(sizes, dtype=float)\n",
    "    times = np.asarray(times, dtype=float)\n",
    "\n",
    "    num_pkts = sizes.size\n",
    "    total_bytes = float(sizes.sum())\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Basic size stats (shape of the payload distribution)\n",
    "    # ------------------------------------------------------------\n",
    "    size_mean, size_std, size_skew, size_kurt, size_cv = _safe_moments(sizes)\n",
    "    size_min = float(sizes.min())\n",
    "    size_max = float(sizes.max())\n",
    "    size_median = float(np.median(sizes))\n",
    "    size_q10 = float(np.percentile(sizes, 10))\n",
    "    size_q25 = float(np.percentile(sizes, 25))\n",
    "    size_q75 = float(np.percentile(sizes, 75))\n",
    "    size_q90 = float(np.percentile(sizes, 90))\n",
    "    size_range = float(size_max - size_min)\n",
    "    size_iqr = float(size_q75 - size_q25)\n",
    "    size_max_over_mean = float(size_max / size_mean) if size_mean != 0 else 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Duration & time normalization\n",
    "    # ------------------------------------------------------------\n",
    "    if num_pkts > 1:\n",
    "        t_min = times.min()\n",
    "        t_max = times.max()\n",
    "        duration = float(t_max - t_min)\n",
    "    else:\n",
    "        t_min = times[0]\n",
    "        t_max = times[0]\n",
    "        duration = 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # IAT (inter-arrival times) stats\n",
    "    # ------------------------------------------------------------\n",
    "    if num_pkts > 1:\n",
    "        sorted_times = np.sort(times)\n",
    "        iats = np.diff(sorted_times)\n",
    "        iat_mean, iat_std, iat_skew, iat_kurt, iat_cv = _safe_moments(iats)\n",
    "        iat_min = float(iats.min())\n",
    "        iat_max = float(iats.max())\n",
    "        iat_median = float(np.median(iats))\n",
    "        iat_q10 = float(np.percentile(iats, 10))\n",
    "        iat_q25 = float(np.percentile(iats, 25))\n",
    "        iat_q75 = float(np.percentile(iats, 75))\n",
    "        iat_q90 = float(np.percentile(iats, 90))\n",
    "        iat_range = float(iat_max - iat_min)\n",
    "        iat_iqr = float(iat_q75 - iat_q25)\n",
    "        iat_max_over_mean = float(iat_max / iat_mean) if iat_mean != 0 else 0.0\n",
    "    else:\n",
    "        iat_mean = iat_std = iat_skew = iat_kurt = iat_cv = 0.0\n",
    "        iat_min = iat_max = iat_median = 0.0\n",
    "        iat_q10 = iat_q25 = iat_q75 = iat_q90 = 0.0\n",
    "        iat_range = iat_iqr = iat_max_over_mean = 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Rates (avoid divide-by-zero if duration == 0)\n",
    "    # ------------------------------------------------------------\n",
    "    if duration > 0:\n",
    "        pkts_per_sec = num_pkts / duration\n",
    "        bytes_per_sec = total_bytes / duration\n",
    "    else:\n",
    "        pkts_per_sec = 0.0\n",
    "        bytes_per_sec = 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Coarse packet size histogram (proportions)\n",
    "    # ------------------------------------------------------------\n",
    "    bin_0_100    = np.sum(sizes <= 100)\n",
    "    bin_100_300  = np.sum((sizes > 100) & (sizes <= 300))\n",
    "    bin_300_600  = np.sum((sizes > 300) & (sizes <= 600))\n",
    "    bin_600_1000 = np.sum((sizes > 600) & (sizes <= 1000))\n",
    "    bin_gt_1000  = np.sum(sizes > 1000)\n",
    "\n",
    "    prop_size_0_100    = bin_0_100    / num_pkts\n",
    "    prop_size_100_300  = bin_100_300  / num_pkts\n",
    "    prop_size_300_600  = bin_300_600  / num_pkts\n",
    "    prop_size_600_1000 = bin_600_1000 / num_pkts\n",
    "    prop_size_gt_1000  = bin_gt_1000  / num_pkts\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Temporal / burstiness features\n",
    "    # ------------------------------------------------------------\n",
    "    if duration > 0 and num_pkts > 1:\n",
    "        rel_t = (times - t_min) / duration  # normalized to [0, 1]\n",
    "\n",
    "        # concentration in early vs late parts\n",
    "        mask_first_20 = rel_t <= 0.2\n",
    "        mask_first_50 = rel_t <= 0.5\n",
    "        mask_last_20  = rel_t >= 0.8\n",
    "\n",
    "        prop_pkts_first_20 = float(mask_first_20.mean())\n",
    "        prop_pkts_first_50 = float(mask_first_50.mean())\n",
    "        prop_pkts_last_20  = float(mask_last_20.mean())\n",
    "\n",
    "        if total_bytes > 0:\n",
    "            prop_bytes_first_20 = float(sizes[mask_first_20].sum() / total_bytes)\n",
    "            prop_bytes_first_50 = float(sizes[mask_first_50].sum() / total_bytes)\n",
    "            prop_bytes_last_20  = float(sizes[mask_last_20].sum() / total_bytes)\n",
    "        else:\n",
    "            prop_bytes_first_20 = prop_bytes_first_50 = prop_bytes_last_20 = 0.0\n",
    "\n",
    "        # Fano factor of packet counts across 10 equal time bins\n",
    "        bins = np.linspace(0.0, 1.0, 11)  # 10 bins\n",
    "        counts, _ = np.histogram(rel_t, bins=bins)\n",
    "        mean_c = counts.mean()\n",
    "        var_c = counts.var()\n",
    "        time_fano_factor = float(var_c / mean_c) if mean_c > 0 else 0.0\n",
    "\n",
    "        # Fano factor for bytes in each bin\n",
    "        bin_idx = np.clip(np.floor(rel_t * 10).astype(int), 0, 9)\n",
    "        byte_counts = np.zeros(10, dtype=float)\n",
    "        for b, idx in zip(sizes, bin_idx):\n",
    "            byte_counts[idx] += b\n",
    "        mean_b = byte_counts.mean()\n",
    "        var_b = byte_counts.var()\n",
    "        bytes_fano_factor = float(var_b / mean_b) if mean_b > 0 else 0.0\n",
    "    else:\n",
    "        prop_pkts_first_20 = prop_pkts_first_50 = prop_pkts_last_20 = 0.0\n",
    "        prop_bytes_first_20 = prop_bytes_first_50 = prop_bytes_last_20 = 0.0\n",
    "        time_fano_factor = 0.0\n",
    "        bytes_fano_factor = 0.0\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Aggregate all features into a dict\n",
    "    # ------------------------------------------------------------\n",
    "    feats = {\n",
    "        # basic counts / volume\n",
    "        \"num_pkts\": float(num_pkts),\n",
    "        \"total_bytes\": float(total_bytes),\n",
    "        \"duration\": float(duration),\n",
    "\n",
    "        # size stats\n",
    "        \"size_mean\": size_mean,\n",
    "        \"size_std\": size_std,\n",
    "        \"size_skew\": size_skew,\n",
    "        \"size_kurt\": size_kurt,\n",
    "        \"size_cv\": size_cv,\n",
    "        \"size_min\": size_min,\n",
    "        \"size_max\": size_max,\n",
    "        \"size_median\": size_median,\n",
    "        \"size_q10\": size_q10,\n",
    "        \"size_q25\": size_q25,\n",
    "        \"size_q75\": size_q75,\n",
    "        \"size_q90\": size_q90,\n",
    "        \"size_range\": size_range,\n",
    "        \"size_iqr\": size_iqr,\n",
    "        \"size_max_over_mean\": size_max_over_mean,\n",
    "\n",
    "        # IAT stats\n",
    "        \"iat_mean\": iat_mean,\n",
    "        \"iat_std\": iat_std,\n",
    "        \"iat_skew\": iat_skew,\n",
    "        \"iat_kurt\": iat_kurt,\n",
    "        \"iat_cv\": iat_cv,\n",
    "        \"iat_min\": iat_min,\n",
    "        \"iat_max\": iat_max,\n",
    "        \"iat_median\": iat_median,\n",
    "        \"iat_q10\": iat_q10,\n",
    "        \"iat_q25\": iat_q25,\n",
    "        \"iat_q75\": iat_q75,\n",
    "        \"iat_q90\": iat_q90,\n",
    "        \"iat_range\": iat_range,\n",
    "        \"iat_iqr\": iat_iqr,\n",
    "        \"iat_max_over_mean\": iat_max_over_mean,\n",
    "\n",
    "        # rates\n",
    "        \"pkts_per_sec\": float(pkts_per_sec),\n",
    "        \"bytes_per_sec\": float(bytes_per_sec),\n",
    "\n",
    "        # size histogram proportions\n",
    "        \"prop_size_0_100\": float(prop_size_0_100),\n",
    "        \"prop_size_100_300\": float(prop_size_100_300),\n",
    "        \"prop_size_300_600\": float(prop_size_300_600),\n",
    "        \"prop_size_600_1000\": float(prop_size_600_1000),\n",
    "        \"prop_size_gt_1000\": float(prop_size_gt_1000),\n",
    "\n",
    "        # temporal concentration / burstiness\n",
    "        \"prop_pkts_first_20\": prop_pkts_first_20,\n",
    "        \"prop_pkts_first_50\": prop_pkts_first_50,\n",
    "        \"prop_pkts_last_20\": prop_pkts_last_20,\n",
    "        \"prop_bytes_first_20\": prop_bytes_first_20,\n",
    "        \"prop_bytes_first_50\": prop_bytes_first_50,\n",
    "        \"prop_bytes_last_20\": prop_bytes_last_20,\n",
    "        \"time_fano_factor\": time_fano_factor,\n",
    "        \"bytes_fano_factor\": bytes_fano_factor,\n",
    "    }\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "feature_rows = []\n",
    "label_rows = []\n",
    "\n",
    "for tsample in pcapml_fe.sampler(str(PCAP_PATH)):\n",
    "    # -----------------------------------------------------------------\n",
    "    # IDs and labels\n",
    "    # -----------------------------------------------------------------\n",
    "    sid = str(tsample.sid)\n",
    "\n",
    "    # Metadata string is like: \"p2p_torrent_torrent\"\n",
    "    meta = str(tsample.metadata).strip()\n",
    "    parts = meta.split(\"_\")\n",
    "\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Unexpected metadata format for sample {sid}: {meta}\")\n",
    "\n",
    "    easy_lbl, med_lbl, hard_lbl = parts  # top-level, mid-level, fine-grained\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Feature extraction\n",
    "    # -----------------------------------------------------------------\n",
    "    feats = extract_features_from_sample(tsample)\n",
    "    feats[\"sampleID\"] = sid\n",
    "    feature_rows.append(feats)\n",
    "\n",
    "    label_rows.append(\n",
    "        {\n",
    "            \"sampleID\": sid,\n",
    "            \"easy_label\": easy_lbl,\n",
    "            \"medium_label\": med_lbl,\n",
    "            \"hard_label\": hard_lbl,\n",
    "        }\n",
    "    )\n",
    "\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "labels_df = pd.DataFrame(label_rows)\n",
    "\n",
    "print(\"Feature frame shape:\", features_df.shape)\n",
    "print(\"Label frame shape:  \", labels_df.shape)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Merge features + labels into a single table\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "full_df = features_df.merge(labels_df, on=\"sampleID\", how=\"inner\")\n",
    "print(\"Merged frame shape:\", full_df.shape)\n",
    "\n",
    "# Save to disk for later experiments\n",
    "OUTPUT_CSV = BASE_DIR / \"application_id_dataset.csv\"\n",
    "full_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved merged dataset to: {OUTPUT_CSV}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pkts</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>duration</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_std</th>\n",
       "      <th>size_skew</th>\n",
       "      <th>size_kurt</th>\n",
       "      <th>size_cv</th>\n",
       "      <th>size_min</th>\n",
       "      <th>size_max</th>\n",
       "      <th>...</th>\n",
       "      <th>prop_pkts_last_20</th>\n",
       "      <th>prop_bytes_first_20</th>\n",
       "      <th>prop_bytes_first_50</th>\n",
       "      <th>prop_bytes_last_20</th>\n",
       "      <th>time_fano_factor</th>\n",
       "      <th>bytes_fano_factor</th>\n",
       "      <th>sampleID</th>\n",
       "      <th>easy_label</th>\n",
       "      <th>medium_label</th>\n",
       "      <th>hard_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9868669216672554899</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15379293250252091038</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>91031.0</td>\n",
       "      <td>431104.0</td>\n",
       "      <td>910.31</td>\n",
       "      <td>645.437552</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>1.31074</td>\n",
       "      <td>0.709030</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.151718</td>\n",
       "      <td>0.533884</td>\n",
       "      <td>26.66</td>\n",
       "      <td>27811.320154</td>\n",
       "      <td>8149511148527902631</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>286720.0</td>\n",
       "      <td>254.50</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.418468</td>\n",
       "      <td>148.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.290766</td>\n",
       "      <td>0.290766</td>\n",
       "      <td>0.709234</td>\n",
       "      <td>0.80</td>\n",
       "      <td>248.166798</td>\n",
       "      <td>16847835362422566935</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>425984.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>51.200000</td>\n",
       "      <td>13772236344740749544</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_pkts  total_bytes  duration  size_mean    size_std  size_skew  \\\n",
       "0       1.0        145.0       0.0     145.00    0.000000    0.00000   \n",
       "1       1.0         60.0       0.0      60.00    0.000000    0.00000   \n",
       "2     100.0      91031.0  431104.0     910.31  645.437552   -0.54859   \n",
       "3       2.0        509.0  286720.0     254.50  106.500000    0.00000   \n",
       "4       2.0        128.0  425984.0      64.00    0.000000    0.00000   \n",
       "\n",
       "   size_kurt   size_cv  size_min  size_max  ...  prop_pkts_last_20  \\\n",
       "0    0.00000  0.000000     145.0     145.0  ...                0.0   \n",
       "1    0.00000  0.000000      60.0      60.0  ...                0.0   \n",
       "2    1.31074  0.709030      54.0    1404.0  ...                0.5   \n",
       "3    1.00000  0.418468     148.0     361.0  ...                0.5   \n",
       "4    0.00000  0.000000      64.0      64.0  ...                0.5   \n",
       "\n",
       "   prop_bytes_first_20  prop_bytes_first_50  prop_bytes_last_20  \\\n",
       "0             0.000000             0.000000            0.000000   \n",
       "1             0.000000             0.000000            0.000000   \n",
       "2             0.000725             0.151718            0.533884   \n",
       "3             0.290766             0.290766            0.709234   \n",
       "4             0.500000             0.500000            0.500000   \n",
       "\n",
       "   time_fano_factor  bytes_fano_factor              sampleID  easy_label  \\\n",
       "0              0.00           0.000000   9868669216672554899         p2p   \n",
       "1              0.00           0.000000  15379293250252091038         p2p   \n",
       "2             26.66       27811.320154   8149511148527902631         p2p   \n",
       "3              0.80         248.166798  16847835362422566935         p2p   \n",
       "4              0.80          51.200000  13772236344740749544         p2p   \n",
       "\n",
       "   medium_label  hard_label  \n",
       "0       torrent     torrent  \n",
       "1       torrent     torrent  \n",
       "2       torrent     torrent  \n",
       "3       torrent     torrent  \n",
       "4       torrent     torrent  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = full_df\n",
    "#df = pd.read_csv(\"application_id_dataset.csv\")  # path from your console output\n",
    "df.head()  # show first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New total feature count: 67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pkts</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>duration</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_std</th>\n",
       "      <th>size_skew</th>\n",
       "      <th>size_kurt</th>\n",
       "      <th>size_cv</th>\n",
       "      <th>size_min</th>\n",
       "      <th>size_max</th>\n",
       "      <th>...</th>\n",
       "      <th>tail_bytes_minus_head</th>\n",
       "      <th>head_bytes_concentration</th>\n",
       "      <th>burstiness_interaction</th>\n",
       "      <th>is_high_burst</th>\n",
       "      <th>is_single_pkt</th>\n",
       "      <th>is_short_flow</th>\n",
       "      <th>is_long_flow</th>\n",
       "      <th>easy_label</th>\n",
       "      <th>medium_label</th>\n",
       "      <th>hard_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>91031.0</td>\n",
       "      <td>431104.0</td>\n",
       "      <td>910.31</td>\n",
       "      <td>645.437552</td>\n",
       "      <td>-0.54859</td>\n",
       "      <td>1.31074</td>\n",
       "      <td>0.709030</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533159</td>\n",
       "      <td>-0.382167</td>\n",
       "      <td>741449.795294</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>286720.0</td>\n",
       "      <td>254.50</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.418468</td>\n",
       "      <td>148.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418468</td>\n",
       "      <td>-0.418468</td>\n",
       "      <td>198.533438</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>425984.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.960000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p2p</td>\n",
       "      <td>torrent</td>\n",
       "      <td>torrent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_pkts  total_bytes  duration  size_mean    size_std  size_skew  \\\n",
       "0       1.0        145.0       0.0     145.00    0.000000    0.00000   \n",
       "1       1.0         60.0       0.0      60.00    0.000000    0.00000   \n",
       "2     100.0      91031.0  431104.0     910.31  645.437552   -0.54859   \n",
       "3       2.0        509.0  286720.0     254.50  106.500000    0.00000   \n",
       "4       2.0        128.0  425984.0      64.00    0.000000    0.00000   \n",
       "\n",
       "   size_kurt   size_cv  size_min  size_max  ...  tail_bytes_minus_head  \\\n",
       "0    0.00000  0.000000     145.0     145.0  ...               0.000000   \n",
       "1    0.00000  0.000000      60.0      60.0  ...               0.000000   \n",
       "2    1.31074  0.709030      54.0    1404.0  ...               0.533159   \n",
       "3    1.00000  0.418468     148.0     361.0  ...               0.418468   \n",
       "4    0.00000  0.000000      64.0      64.0  ...               0.000000   \n",
       "\n",
       "   head_bytes_concentration  burstiness_interaction  is_high_burst  \\\n",
       "0                  0.000000                0.000000              0   \n",
       "1                  0.000000                0.000000              0   \n",
       "2                 -0.382167           741449.795294              1   \n",
       "3                 -0.418468              198.533438              1   \n",
       "4                  0.000000               40.960000              0   \n",
       "\n",
       "   is_single_pkt  is_short_flow  is_long_flow  easy_label  medium_label  \\\n",
       "0              1              1             0         p2p       torrent   \n",
       "1              1              1             0         p2p       torrent   \n",
       "2              0              0             0         p2p       torrent   \n",
       "3              0              1             0         p2p       torrent   \n",
       "4              0              0             0         p2p       torrent   \n",
       "\n",
       "   hard_label  \n",
       "0     torrent  \n",
       "1     torrent  \n",
       "2     torrent  \n",
       "3     torrent  \n",
       "4     torrent  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Copy original dataset\n",
    "df_fe = df.copy()\n",
    "eps = 1e-6  \n",
    "\n",
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "\n",
    "# ======================================================\n",
    "# 1️⃣ Remove labels temporarily while engineering features\n",
    "# ======================================================\n",
    "df_features = df_fe.drop(columns=label_cols)\n",
    "\n",
    "# ---------- Feature Engineering ----------\n",
    "# 1. Log transforms\n",
    "for col in [\"num_pkts\", \"total_bytes\", \"duration\"]:\n",
    "    if col in df_features.columns:\n",
    "        df_features[f\"log_{col}\"] = np.log1p(df_features[col])\n",
    "\n",
    "# 2. Rates \n",
    "if {\"total_bytes\", \"num_pkts\"}.issubset(df_features.columns):\n",
    "    df_features[\"bytes_per_pkt\"] = df_features[\"total_bytes\"] / (df_features[\"num_pkts\"] + eps)\n",
    "\n",
    "if {\"num_pkts\", \"duration\"}.issubset(df_features.columns):\n",
    "    df_features[\"pkts_per_time\"] = df_features[\"num_pkts\"] / (df_features[\"duration\"] + eps)\n",
    "\n",
    "if {\"total_bytes\", \"duration\"}.issubset(df_features.columns):\n",
    "    df_features[\"bytes_per_time\"] = df_features[\"total_bytes\"] / (df_features[\"duration\"] + eps)\n",
    "\n",
    "# 3. Packet size structure\n",
    "needed = {\"size_min\", \"size_max\", \"size_mean\", \"size_std\"}\n",
    "if needed.issubset(df_features.columns):\n",
    "    df_features[\"size_range\"] = df_features[\"size_max\"] - df_features[\"size_min\"]\n",
    "    df_features[\"size_std_over_mean\"] = df_features[\"size_std\"] / (df_features[\"size_mean\"] + eps)\n",
    "    df_features[\"size_mean_over_max\"] = df_features[\"size_mean\"] / (df_features[\"size_max\"] + eps)\n",
    "\n",
    "# 4. Burstiness\n",
    "if {\"prop_bytes_first_20\", \"prop_bytes_last_20\"}.issubset(df_features.columns):\n",
    "    df_features[\"tail_bytes_minus_head\"] = (\n",
    "        df_features[\"prop_bytes_last_20\"] - df_features[\"prop_bytes_first_20\"]\n",
    "    )\n",
    "\n",
    "if {\"prop_bytes_first_50\", \"prop_bytes_last_20\"}.issubset(df_features.columns):\n",
    "    df_features[\"head_bytes_concentration\"] = (\n",
    "        df_features[\"prop_bytes_first_50\"] - df_features[\"prop_bytes_last_20\"]\n",
    "    )\n",
    "\n",
    "if {\"time_fano_factor\", \"bytes_fano_factor\"}.issubset(df_features.columns):\n",
    "    df_features[\"burstiness_interaction\"] = (\n",
    "        df_features[\"time_fano_factor\"] * df_features[\"bytes_fano_factor\"]\n",
    "    )\n",
    "\n",
    "    time_thr = df_features[\"time_fano_factor\"].quantile(0.9)\n",
    "    bytes_thr = df_features[\"bytes_fano_factor\"].quantile(0.9)\n",
    "\n",
    "    df_features[\"is_high_burst\"] = (\n",
    "        (df_features[\"time_fano_factor\"] > time_thr) |\n",
    "        (df_features[\"bytes_fano_factor\"] > bytes_thr)\n",
    "    ).astype(int)\n",
    "\n",
    "# 5. Binary flags\n",
    "if \"num_pkts\" in df_features.columns:\n",
    "    df_features[\"is_single_pkt\"] = (df_features[\"num_pkts\"] == 1).astype(int)\n",
    "\n",
    "if \"duration\" in df_features.columns:\n",
    "    q1 = df_features[\"duration\"].quantile(0.25)\n",
    "    q3 = df_features[\"duration\"].quantile(0.75)\n",
    "\n",
    "    df_features[\"is_short_flow\"] = (df_features[\"duration\"] <= q1).astype(int)\n",
    "    df_features[\"is_long_flow\"] = (df_features[\"duration\"] >= q3).astype(int)\n",
    "\n",
    "# ======================================================\n",
    "# 2️⃣ Reattach labels to the END of the dataframe\n",
    "# ======================================================\n",
    "df = pd.concat([df_features, df_fe[label_cols]], axis=1)\n",
    "\n",
    "print(\"New total feature count:\", df.shape[1])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Class Counts for easy_label ===\n",
      "easy_label\n",
      "audio            113150\n",
      "chat               5070\n",
      "email              2898\n",
      "file-transfer     32114\n",
      "p2p                1045\n",
      "tor                 109\n",
      "video              3969\n",
      "Name: count, dtype: int64\n",
      "Total unique classes: 7\n",
      "Total samples: 158355\n",
      "\n",
      "=== Class Counts for medium_label ===\n",
      "medium_label\n",
      "aim             409\n",
      "email          2898\n",
      "facebook      44366\n",
      "ftps            750\n",
      "gmail           446\n",
      "google            5\n",
      "hangouts      47433\n",
      "icq             434\n",
      "netflix         255\n",
      "scp             170\n",
      "sftp            188\n",
      "skype         55618\n",
      "spotify         204\n",
      "torrent        1045\n",
      "twitter           6\n",
      "vimeo           422\n",
      "voipbuster     2773\n",
      "youtube         933\n",
      "Name: count, dtype: int64\n",
      "Total unique classes: 18\n",
      "Total samples: 158355\n",
      "\n",
      "=== Class Counts for hard_label ===\n",
      "hard_label\n",
      "aim-chat            409\n",
      "email              2898\n",
      "facebook-audio    43454\n",
      "facebook-chat       505\n",
      "facebook-video      402\n",
      "ftps-down           606\n",
      "ftps-up             144\n",
      "gmail-chat          446\n",
      "hangouts-audio    45588\n",
      "hangouts-chat       338\n",
      "hangouts-video     1507\n",
      "icq-chat            434\n",
      "netflix             255\n",
      "scp-down             86\n",
      "scp-up               84\n",
      "sftp-down            79\n",
      "sftp-up             109\n",
      "skype-audio       21131\n",
      "skype-chat         2938\n",
      "skype-file        31006\n",
      "skype-video         543\n",
      "spotify             204\n",
      "tor-facebook          5\n",
      "tor-google            5\n",
      "tor-twitter           6\n",
      "tor-vimeo            12\n",
      "tor-youtube          93\n",
      "torrent            1045\n",
      "vimeo               410\n",
      "voipbuster         2773\n",
      "youtube             840\n",
      "Name: count, dtype: int64\n",
      "Total unique classes: 31\n",
      "Total samples: 158355\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset you just created\n",
    "#df = pd.read_csv(\"application_id_dataset.csv\")\n",
    "\n",
    "# Columns you want to summarize\n",
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "\n",
    "for col in label_cols:\n",
    "    print(f\"\\n=== Class Counts for {col} ===\")\n",
    "    print(df[col].value_counts().sort_index())\n",
    "    print(f\"Total unique classes: {df[col].nunique()}\")\n",
    "    print(f\"Total samples: {df[col].count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "id_cols = [\"sampleID\"] if \"sampleID\" in df.columns else []\n",
    "drop_cols = set(label_cols + id_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 158355\n",
      "Number of features: 63\n",
      "Classes: ['audio' 'chat' 'email' 'file-transfer' 'p2p' 'tor' 'video']\n",
      "Train size: 126684\n",
      "Test size: 31671\n",
      "\n",
      "=== Starting RandomizedSearchCV (light config) ===\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=True, max_depth=20, max_features=0.3, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  52.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 93\u001b[0m\n\u001b[1;32m     81\u001b[0m rf_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     82\u001b[0m     rf_base,\n\u001b[1;32m     83\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED,\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Starting RandomizedSearchCV (light config) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[43mrf_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest CV macro F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rf_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1960\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1960\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Prepare features and labels\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Assume df already exists in memory\n",
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "id_cols = [\"sampleID\"] if \"sampleID\" in df.columns else []\n",
    "drop_cols = set(label_cols + id_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "target_col = \"easy_label\"   # 7-class label\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y_raw = df[target_col].astype(str).values\n",
    "\n",
    "# Encode labels to integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(\"Number of samples:\", X.shape[0])\n",
    "print(\"Number of features:\", X.shape[1])\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Train / test split (80/20, stratified)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Random Forest hyperparameter search (lighter + multi-core)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# IMPORTANT: n_jobs=1 here so CV can parallelize across cores via RandomizedSearchCV\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    n_jobs=1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "# Trimmed, sane search space (you can expand later if needed)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200, 400, 600],          # no 800/1000 to start\n",
    "    \"max_depth\": [None, 10, 20, 30],          # drop 40 for now\n",
    "    \"min_samples_split\": [2, 5],              # fewer options\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", 0.3, 0.5],       # no 0.7/log2 for now\n",
    "    \"bootstrap\": [True],                      # drop False (expensive)\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,               # small to start; can increase if needed\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,               # parallelize across folds/params\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Starting RandomizedSearchCV (light config) ===\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest params:\", rf_search.best_params_)\n",
    "print(\"Best CV macro F1:\", rf_search.best_score_)\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Evaluation on held-out test set\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "y_pred = rf_best.predict(X_test)\n",
    "y_proba = rf_best.predict_proba(X_test)\n",
    "\n",
    "# F1 scores\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "f1_micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"\\nTest F1 (macro):   {f1_macro:.4f}\")\n",
    "print(f\"Test F1 (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"Test F1 (micro):    {f1_micro:.4f}\")\n",
    "\n",
    "# mAP (macro)\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
    "mAP_macro = average_precision_score(y_test_bin, y_proba, average=\"macro\")\n",
    "print(f\"Test mAP (macro):  {mAP_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (per class):\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Confusion matrix (normalized + counts)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix (row-normalized)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix (counts)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Feature importance\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "importances = rf_best.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "top_k = 20\n",
    "print(\"\\nTop feature importances:\")\n",
    "for i in range(min(top_k, len(feature_cols))):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1:2d}. {feature_cols[idx]:30s}  {importances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# micro AP\n",
    "map_micro = average_precision_score(y_test_bin, y_proba, average=\"micro\")\n",
    "print(\"Test mAP (micro):\", map_micro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Prepare features and labels for MEDIUM level\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# df is assumed to already include your engineered features\n",
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "id_cols = [\"sampleID\"] if \"sampleID\" in df.columns else []\n",
    "drop_cols = set(label_cols + id_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "target_col = \"medium_label\"   # 31-class label\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y_raw = df[target_col].astype(str).values\n",
    "\n",
    "# Encode labels\n",
    "le_mid = LabelEncoder()\n",
    "y = le_mid.fit_transform(y_raw)\n",
    "class_names = le_mid.classes_\n",
    "\n",
    "print(\"=== Medium-level task ===\")\n",
    "print(\"Number of samples:\", X.shape[0])\n",
    "print(\"Number of features:\", X.shape[1])\n",
    "print(\"Num classes:\", len(class_names))\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Train / test split (80/20, stratified)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Random Forest hyperparameter search (medium level)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Start near the good region we found for easy_label, but keep it light\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    n_jobs=1,                     # let RandomizedSearchCV parallelize\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "param_dist_mid = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [20, 30, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", 0.3],\n",
    "    \"bootstrap\": [True],\n",
    "}\n",
    "\n",
    "rf_search_mid = RandomizedSearchCV(\n",
    "    rf_base,\n",
    "    param_distributions=param_dist_mid,\n",
    "    n_iter=12,               # can bump to 20 if still fast enough\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Starting RandomizedSearchCV for medium_label ===\")\n",
    "rf_search_mid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest params (medium):\", rf_search_mid.best_params_)\n",
    "print(\"Best CV macro F1 (medium):\", rf_search_mid.best_score_)\n",
    "\n",
    "rf_best_mid = rf_search_mid.best_estimator_\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Evaluation on held-out test set (medium level)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "y_pred = rf_best_mid.predict(X_test)\n",
    "y_proba = rf_best_mid.predict_proba(X_test)\n",
    "\n",
    "# F1 scores\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "f1_micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"\\n[Medium] Test F1 (macro):   {f1_macro:.4f}\")\n",
    "print(f\"[Medium] Test F1 (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"[Medium] Test F1 (micro):    {f1_micro:.4f}\")\n",
    "\n",
    "# mAP (macro)\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
    "mAP_macro = average_precision_score(y_test_bin, y_proba, average=\"macro\")\n",
    "print(f\"[Medium] Test mAP (macro):  {mAP_macro:.4f}\")\n",
    "\n",
    "print(\"\\n[Medium] Classification report (per class):\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Confusion matrix (might be large: 31x31)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=False,  # True will be tiny; turn on if you zoom\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix (row-normalized) – medium_label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Feature importance for medium level\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "importances = rf_best_mid.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "top_k = 20\n",
    "print(\"\\n[Medium] Top feature importances:\")\n",
    "for i in range(min(top_k, len(feature_cols))):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1:2d}. {feature_cols[idx]:30s}  {importances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Prepare features and labels for HARD level\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# df is assumed to already include your engineered features\n",
    "label_cols = [\"easy_label\", \"medium_label\", \"hard_label\"]\n",
    "id_cols = [\"sampleID\"] if \"sampleID\" in df.columns else []\n",
    "drop_cols = set(label_cols + id_cols)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "target_col = \"hard_label\"   # fine-grained label\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y_raw = df[target_col].astype(str).values\n",
    "\n",
    "# Encode hard labels\n",
    "le_hard = LabelEncoder()\n",
    "y = le_hard.fit_transform(y_raw)\n",
    "class_names = le_hard.classes_\n",
    "\n",
    "print(\"=== Hard-level task ===\")\n",
    "print(\"Number of samples:\", X.shape[0])\n",
    "print(\"Number of features:\", X.shape[1])\n",
    "print(\"Num classes:\", len(class_names))\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Train / test split (80/20, stratified)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Random Forest hyperparameter search (HARD level)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Hard level is very imbalanced; keep class_weight on\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    n_jobs=1,                     # let RandomizedSearchCV parallelize\n",
    "    random_state=RANDOM_SEED,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "param_dist_hard = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [20, 30, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", 0.3],\n",
    "    \"bootstrap\": [True],\n",
    "}\n",
    "\n",
    "rf_search_hard = RandomizedSearchCV(\n",
    "    rf_base,\n",
    "    param_distributions=param_dist_hard,\n",
    "    n_iter=12,               # bump to 20 if you want, once you see runtime\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Starting RandomizedSearchCV for hard_label ===\")\n",
    "rf_search_hard.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest params (hard):\", rf_search_hard.best_params_)\n",
    "print(\"Best CV macro F1 (hard):\", rf_search_hard.best_score_)\n",
    "\n",
    "rf_best_hard = rf_search_hard.best_estimator_\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Evaluation on held-out test set (hard level)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "y_pred = rf_best_hard.predict(X_test)\n",
    "y_proba = rf_best_hard.predict_proba(X_test)\n",
    "\n",
    "# F1 scores\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "f1_micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"\\n[Hard] Test F1 (macro):   {f1_macro:.4f}\")\n",
    "print(f\"[Hard] Test F1 (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"[Hard] Test F1 (micro):    {f1_micro:.4f}\")\n",
    "\n",
    "# mAP (macro)\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(len(class_names)))\n",
    "mAP_macro = average_precision_score(y_test_bin, y_proba, average=\"macro\")\n",
    "print(f\"[Hard] Test mAP (macro):  {mAP_macro:.4f}\")\n",
    "\n",
    "print(\"\\n[Hard] Classification report (per class):\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Confusion matrix (can be big; you may want to subset to top classes)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=False,  # True gets unreadable with many classes\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix (row-normalized) – hard_label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Feature importance for hard level\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "importances = rf_best_hard.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "top_k = 20\n",
    "print(\"\\n[Hard] Top feature importances:\")\n",
    "for i in range(min(top_k, len(feature_cols))):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1:2d}. {feature_cols[idx]:30s}  {importances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM With Regular Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "def netml_split_80_10_10(y, random_state=0):\n",
    "    \"\"\"\n",
    "    Implement the NetML style split:\n",
    "      - For each class separately:\n",
    "        * randomly select 10 percent of samples to test_chal\n",
    "        * randomly select 10 percent to test_std\n",
    "        * remaining 80 percent to train\n",
    "    Returns three index arrays: idx_train, idx_test_std, idx_test_chal\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    y = np.asarray(y)\n",
    "    unique_classes = np.unique(y)\n",
    "\n",
    "    idx_train = []\n",
    "    idx_test_std = []\n",
    "    idx_test_chal = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        n = len(cls_idx)\n",
    "        # shuffle indices for this class\n",
    "        perm = rng.permutation(cls_idx)\n",
    "\n",
    "        # number for each split (floor so small classes can end up with zero in a test split)\n",
    "        n_chal = int(0.1 * n)\n",
    "        n_std = int(0.1 * n)\n",
    "\n",
    "        chal_idx = perm[:n_chal]\n",
    "        std_idx = perm[n_chal:n_chal + n_std]\n",
    "        train_idx = perm[n_chal + n_std:]\n",
    "\n",
    "        idx_test_chal.append(chal_idx)\n",
    "        idx_test_std.append(std_idx)\n",
    "        idx_train.append(train_idx)\n",
    "\n",
    "    idx_train = np.concatenate(idx_train)\n",
    "    idx_test_std = np.concatenate(idx_test_std)\n",
    "    idx_test_chal = np.concatenate(idx_test_chal)\n",
    "\n",
    "    return idx_train, idx_test_std, idx_test_chal\n",
    "\n",
    "\n",
    "def train_eval_svm_for_label(df, feature_cols, label_col, random_state=0):\n",
    "    \"\"\"\n",
    "    Match the paper:\n",
    "      - Per class 80 percent train, 10 percent test_std, 10 percent test_chal\n",
    "      - Only 10 percent of the training set is used to fit SVM, remaining 90 percent is validation\n",
    "      - RBF SVM with C = 1.0, gamma = 'scale'\n",
    "      - Report F1 (macro, weighted, micro) and mAP (macro) on validation and test_std\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Build X, y\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "\n",
    "    # 2. NetML style 80 / 10 / 10 split per class\n",
    "    idx_train, idx_test_std, idx_test_chal = netml_split_80_10_10(y, random_state=random_state)\n",
    "\n",
    "    X_train_full = X[idx_train]\n",
    "    y_train_full = y[idx_train]\n",
    "\n",
    "    X_test_std = X[idx_test_std]\n",
    "    y_test_std = y[idx_test_std]\n",
    "\n",
    "    X_test_chal = X[idx_test_chal]\n",
    "    y_test_chal = y[idx_test_chal]\n",
    "\n",
    "    print(f\"[{label_col}] total samples: {len(df)}\")\n",
    "    print(\n",
    "        f\"[{label_col}] train: {len(X_train_full)}, \"\n",
    "        f\"test_std: {len(X_test_std)}, \"\n",
    "        f\"test_chal: {len(X_test_chal)}\"\n",
    "    )\n",
    "\n",
    "    # 3. Inside training set, take random 10 percent for fitting SVM, 90 percent for validation\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_train = len(X_train_full)\n",
    "    n_fit = max(1, int(0.1 * n_train))\n",
    "\n",
    "    perm_train = rng.permutation(n_train)\n",
    "    idx_fit_local = perm_train[:n_fit]\n",
    "    idx_val_local = perm_train[n_fit:]\n",
    "\n",
    "    X_fit = X_train_full[idx_fit_local]\n",
    "    y_fit = y_train_full[idx_fit_local]\n",
    "\n",
    "    X_val = X_train_full[idx_val_local]\n",
    "    y_val = y_train_full[idx_val_local]\n",
    "\n",
    "    print(f\"[{label_col}] SVM fit subset: {len(X_fit)} (10 percent of train)\")\n",
    "    print(f\"[{label_col}] validation subset: {len(X_val)} (90 percent of train)\")\n",
    "\n",
    "    # 4. SVM pipeline: StandardScaler + RBF SVM\n",
    "    svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=1.0,\n",
    "            gamma=\"scale\",\n",
    "            probability=True,  # needed for mAP\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 5. Train on the 10 percent fit subset\n",
    "    svm_clf.fit(X_fit, y_fit)\n",
    "\n",
    "    def compute_metrics(model, X_split, y_split, split_name):\n",
    "        \"\"\"\n",
    "        Compute F1 macro, weighted, micro and mAP macro for a split.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_split)\n",
    "\n",
    "        f1_macro = f1_score(y_split, y_pred, average=\"macro\")\n",
    "        f1_weighted = f1_score(y_split, y_pred, average=\"weighted\")\n",
    "        f1_micro = f1_score(y_split, y_pred, average=\"micro\")\n",
    "\n",
    "        # mAP (macro) one versus rest\n",
    "        classes = np.unique(y_fit)  # classes seen in training\n",
    "        y_true_bin = label_binarize(y_split, classes=classes)\n",
    "        y_scores = model.predict_proba(X_split)\n",
    "\n",
    "        # Some classes might not appear in this split; average=\"macro\" handles that\n",
    "        mAP_macro = average_precision_score(y_true_bin, y_scores, average=\"macro\")\n",
    "\n",
    "        print(f\"\\n[{label_col}] {split_name} F1 (macro):     {f1_macro:.4f}\")\n",
    "        print(f\"[{label_col}] {split_name} F1 (weighted):  {f1_weighted:.4f}\")\n",
    "        print(f\"[{label_col}] {split_name} F1 (micro):     {f1_micro:.4f}\")\n",
    "        print(f\"[{label_col}] {split_name} mAP (macro):    {mAP_macro:.4f}\")\n",
    "\n",
    "        return {\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"mAP_macro\": mAP_macro,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "\n",
    "    # 6. Metrics on validation and test_std\n",
    "    val_metrics = compute_metrics(svm_clf, X_val, y_val, \"validation\")\n",
    "    test_std_metrics = compute_metrics(svm_clf, X_test_std, y_test_std, \"test_std\")\n",
    "\n",
    "    print(\"\\nValidation classification report:\")\n",
    "    print(classification_report(y_val, val_metrics[\"y_pred\"]))\n",
    "\n",
    "    print(\"\\nTest_std classification report:\")\n",
    "    print(classification_report(y_test_std, test_std_metrics[\"y_pred\"]))\n",
    "\n",
    "    cm_test_std = confusion_matrix(y_test_std, test_std_metrics[\"y_pred\"])\n",
    "\n",
    "    return {\n",
    "        \"model\": svm_clf,\n",
    "        \"idx_train\": idx_train,\n",
    "        \"idx_test_std\": idx_test_std,\n",
    "        \"idx_test_chal\": idx_test_chal,\n",
    "        \"X_fit\": X_fit,\n",
    "        \"y_fit\": y_fit,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test_std\": X_test_std,\n",
    "        \"y_test_std\": y_test_std,\n",
    "        \"X_test_chal\": X_test_chal,\n",
    "        \"y_test_chal\": y_test_chal,\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_std_metrics\": test_std_metrics,\n",
    "        \"cm_test_std\": cm_test_std,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy_label] total samples: 158355\n",
      "[easy_label] train: 126691, test_std: 15832, test_chal: 15832\n",
      "[easy_label] SVM fit subset: 12669 (10 percent of train)\n",
      "[easy_label] validation subset: 114022 (90 percent of train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[easy_label] validation F1 (macro):     0.2415\n",
      "[easy_label] validation F1 (weighted):  0.6165\n",
      "[easy_label] validation F1 (micro):     0.7220\n",
      "[easy_label] validation mAP (macro):    0.2901\n",
      "\n",
      "[easy_label] test_std F1 (macro):     0.2310\n",
      "[easy_label] test_std F1 (weighted):  0.6149\n",
      "[easy_label] test_std F1 (micro):     0.7211\n",
      "[easy_label] test_std mAP (macro):    0.2874\n",
      "\n",
      "Validation classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        audio       0.72      1.00      0.84     81459\n",
      "         chat       0.93      0.07      0.13      3652\n",
      "        email       0.74      0.06      0.10      2084\n",
      "file-transfer       0.75      0.01      0.01     23144\n",
      "          p2p       0.67      0.19      0.30       764\n",
      "          tor       0.00      0.00      0.00        76\n",
      "        video       0.52      0.23      0.32      2843\n",
      "\n",
      "     accuracy                           0.72    114022\n",
      "    macro avg       0.62      0.22      0.24    114022\n",
      " weighted avg       0.73      0.72      0.62    114022\n",
      "\n",
      "\n",
      "Test_std classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        audio       0.72      0.99      0.84     11315\n",
      "         chat       0.97      0.06      0.12       507\n",
      "        email       0.72      0.04      0.08       289\n",
      "file-transfer       1.00      0.01      0.01      3211\n",
      "          p2p       0.73      0.18      0.29       104\n",
      "          tor       0.00      0.00      0.00        10\n",
      "        video       0.46      0.20      0.28       396\n",
      "\n",
      "     accuracy                           0.72     15832\n",
      "    macro avg       0.66      0.21      0.23     15832\n",
      " weighted avg       0.78      0.72      0.61     15832\n",
      "\n",
      "[medium_label] total samples: 158355\n",
      "[medium_label] train: 126701, test_std: 15827, test_chal: 15827\n",
      "[medium_label] SVM fit subset: 12670 (10 percent of train)\n",
      "[medium_label] validation subset: 114031 (90 percent of train)\n",
      "\n",
      "[medium_label] validation F1 (macro):     0.1722\n",
      "[medium_label] validation F1 (weighted):  0.3434\n",
      "[medium_label] validation F1 (micro):     0.4251\n",
      "[medium_label] validation mAP (macro):    0.2311\n",
      "\n",
      "[medium_label] test_std F1 (macro):     0.1875\n",
      "[medium_label] test_std F1 (weighted):  0.3472\n",
      "[medium_label] test_std F1 (micro):     0.4274\n",
      "[medium_label] test_std mAP (macro):    0.2370\n",
      "\n",
      "Validation classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         aim       0.00      0.00      0.00       290\n",
      "       email       0.89      0.05      0.10      2088\n",
      "    facebook       0.47      0.13      0.20     31993\n",
      "        ftps       0.40      0.21      0.27       532\n",
      "       gmail       0.86      0.43      0.57       317\n",
      "      google       0.00      0.00      0.00         5\n",
      "    hangouts       0.74      0.16      0.26     34155\n",
      "         icq       0.00      0.00      0.00       322\n",
      "     netflix       1.00      0.01      0.02       183\n",
      "         scp       0.00      0.00      0.00       119\n",
      "        sftp       1.00      0.12      0.21       142\n",
      "       skype       0.40      0.95      0.56     39999\n",
      "     spotify       0.40      0.01      0.03       153\n",
      "     torrent       0.68      0.10      0.18       756\n",
      "     twitter       0.00      0.00      0.00         6\n",
      "       vimeo       0.53      0.17      0.26       302\n",
      "  voipbuster       0.44      0.19      0.26      1990\n",
      "     youtube       0.27      0.13      0.17       679\n",
      "\n",
      "    accuracy                           0.43    114031\n",
      "   macro avg       0.45      0.15      0.17    114031\n",
      "weighted avg       0.53      0.43      0.34    114031\n",
      "\n",
      "\n",
      "Test_std classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         aim       0.00      0.00      0.00        40\n",
      "       email       0.88      0.05      0.09       289\n",
      "    facebook       0.47      0.14      0.21      4436\n",
      "        ftps       0.35      0.19      0.24        75\n",
      "       gmail       0.75      0.34      0.47        44\n",
      "    hangouts       0.74      0.16      0.26      4743\n",
      "         icq       0.00      0.00      0.00        43\n",
      "     netflix       0.00      0.00      0.00        25\n",
      "         scp       0.00      0.00      0.00        17\n",
      "        sftp       1.00      0.06      0.11        18\n",
      "       skype       0.40      0.95      0.56      5561\n",
      "     spotify       1.00      0.10      0.18        20\n",
      "     torrent       0.90      0.09      0.16       104\n",
      "       vimeo       0.57      0.19      0.29        42\n",
      "  voipbuster       0.47      0.21      0.29       277\n",
      "     youtube       0.20      0.11      0.14        93\n",
      "\n",
      "    accuracy                           0.43     15827\n",
      "   macro avg       0.48      0.16      0.19     15827\n",
      "weighted avg       0.53      0.43      0.35     15827\n",
      "\n",
      "[hard_label] total samples: 158355\n",
      "[hard_label] train: 126715, test_std: 15820, test_chal: 15820\n",
      "[hard_label] SVM fit subset: 12671 (10 percent of train)\n",
      "[hard_label] validation subset: 114044 (90 percent of train)\n",
      "\n",
      "[hard_label] validation F1 (macro):     0.1188\n",
      "[hard_label] validation F1 (weighted):  0.2424\n",
      "[hard_label] validation F1 (micro):     0.3182\n",
      "[hard_label] validation mAP (macro):    0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1030: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[hard_label] test_std F1 (macro):     0.1251\n",
      "[hard_label] test_std F1 (weighted):  0.2453\n",
      "[hard_label] test_std F1 (micro):     0.3218\n",
      "[hard_label] test_std mAP (macro):    0.1578\n",
      "\n",
      "Validation classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      aim-chat       0.00      0.00      0.00       290\n",
      "         email       0.45      0.07      0.12      2088\n",
      "facebook-audio       0.31      0.51      0.38     31344\n",
      " facebook-chat       0.70      0.12      0.21       371\n",
      "facebook-video       1.00      0.02      0.04       295\n",
      "     ftps-down       0.44      0.12      0.19       443\n",
      "       ftps-up       0.57      0.04      0.07       100\n",
      "    gmail-chat       0.99      0.51      0.67       325\n",
      "hangouts-audio       0.32      0.58      0.41     32789\n",
      " hangouts-chat       0.00      0.00      0.00       243\n",
      "hangouts-video       0.12      0.01      0.02      1047\n",
      "      icq-chat       0.00      0.00      0.00       312\n",
      "       netflix       0.00      0.00      0.00       187\n",
      "      scp-down       0.00      0.00      0.00        63\n",
      "        scp-up       0.00      0.00      0.00        64\n",
      "     sftp-down       0.00      0.00      0.00        59\n",
      "       sftp-up       1.00      0.01      0.02        83\n",
      "   skype-audio       0.21      0.00      0.01     15182\n",
      "    skype-chat       0.55      0.02      0.05      2127\n",
      "    skype-file       0.21      0.00      0.01     22336\n",
      "   skype-video       0.72      0.09      0.16       396\n",
      "       spotify       0.19      0.09      0.12       152\n",
      "  tor-facebook       0.00      0.00      0.00         5\n",
      "    tor-google       0.00      0.00      0.00         5\n",
      "   tor-twitter       0.00      0.00      0.00         4\n",
      "     tor-vimeo       0.00      0.00      0.00        10\n",
      "   tor-youtube       0.00      0.00      0.00        68\n",
      "       torrent       0.45      0.39      0.42       768\n",
      "         vimeo       0.33      0.30      0.31       290\n",
      "    voipbuster       0.53      0.12      0.19      1985\n",
      "       youtube       0.32      0.24      0.27       613\n",
      "\n",
      "      accuracy                           0.32    114044\n",
      "     macro avg       0.30      0.10      0.12    114044\n",
      "  weighted avg       0.29      0.32      0.24    114044\n",
      "\n",
      "\n",
      "Test_std classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      aim-chat       0.00      0.00      0.00        40\n",
      "         email       0.40      0.06      0.10       289\n",
      "facebook-audio       0.31      0.51      0.39      4345\n",
      " facebook-chat       0.29      0.04      0.07        50\n",
      "facebook-video       1.00      0.03      0.05        40\n",
      "     ftps-down       0.61      0.18      0.28        60\n",
      "       ftps-up       0.00      0.00      0.00        14\n",
      "    gmail-chat       1.00      0.48      0.65        44\n",
      "hangouts-audio       0.32      0.59      0.42      4558\n",
      " hangouts-chat       0.00      0.00      0.00        33\n",
      "hangouts-video       0.30      0.04      0.07       150\n",
      "      icq-chat       0.00      0.00      0.00        43\n",
      "       netflix       0.00      0.00      0.00        25\n",
      "      scp-down       0.00      0.00      0.00         8\n",
      "        scp-up       0.00      0.00      0.00         8\n",
      "     sftp-down       0.00      0.00      0.00         7\n",
      "       sftp-up       0.00      0.00      0.00        10\n",
      "   skype-audio       0.14      0.00      0.01      2113\n",
      "    skype-chat       0.70      0.02      0.05       293\n",
      "    skype-file       0.33      0.01      0.01      3100\n",
      "   skype-video       0.50      0.06      0.10        54\n",
      "       spotify       0.00      0.00      0.00        20\n",
      "     tor-vimeo       0.00      0.00      0.00         1\n",
      "   tor-youtube       0.00      0.00      0.00         9\n",
      "       torrent       0.43      0.37      0.40       104\n",
      "         vimeo       0.41      0.37      0.38        41\n",
      "    voipbuster       0.62      0.12      0.20       277\n",
      "       youtube       0.34      0.32      0.33        84\n",
      "\n",
      "      accuracy                           0.32     15820\n",
      "     macro avg       0.28      0.11      0.13     15820\n",
      "  weighted avg       0.31      0.32      0.25     15820\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "res_easy = train_eval_svm_for_label(df, feature_cols, \"easy_label\")\n",
    "res_medium = train_eval_svm_for_label(df, feature_cols, \"medium_label\")\n",
    "res_hard = train_eval_svm_for_label(df, feature_cols, \"hard_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nPrint Method - potentially do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow labels shape: (158355, 4)\n",
      "nPrint features shape: (7834, 1778)\n",
      "Final shape: (7834, 1781)\n",
      "           src_ip  eth_dhost_0  eth_dhost_1  eth_dhost_2  eth_dhost_3  \\\n",
      "0  131.202.240.87            0            0            0            0   \n",
      "1  131.202.240.87            0            0            0            0   \n",
      "2  131.202.242.93            0            0            0            0   \n",
      "3  131.202.242.93            0            0            0            0   \n",
      "4  131.202.242.93            0            0            0            0   \n",
      "\n",
      "   eth_dhost_4  eth_dhost_5  eth_dhost_6  eth_dhost_7  eth_dhost_8  ...  \\\n",
      "0            0            0            0            1            0  ...   \n",
      "1            0            0            0            1            0  ...   \n",
      "2            0            0            0            1            0  ...   \n",
      "3            0            0            0            1            0  ...   \n",
      "4            0            0            0            1            0  ...   \n",
      "\n",
      "   payload_bit_250  payload_bit_251  payload_bit_252  payload_bit_253  \\\n",
      "0             -1.0             -1.0             -1.0             -1.0   \n",
      "1             -1.0             -1.0             -1.0             -1.0   \n",
      "2             -1.0             -1.0             -1.0             -1.0   \n",
      "3             -1.0             -1.0             -1.0             -1.0   \n",
      "4             -1.0             -1.0             -1.0             -1.0   \n",
      "\n",
      "   payload_bit_254  payload_bit_255               flow_id  easy_label  \\\n",
      "0             -1.0             -1.0  12962054460325634226       audio   \n",
      "1             -1.0             -1.0  12962054460325634226       audio   \n",
      "2             -1.0             -1.0  17705154377992182019       audio   \n",
      "3             -1.0             -1.0  17705154377992182019       audio   \n",
      "4             -1.0             -1.0  17705154377992182019       audio   \n",
      "\n",
      "   medium_label      hard_label  \n",
      "0      facebook  facebook-audio  \n",
      "1      facebook  facebook-audio  \n",
      "2      facebook  facebook-audio  \n",
      "3      facebook  facebook-audio  \n",
      "4      facebook  facebook-audio  \n",
      "\n",
      "[5 rows x 1781 columns]\n",
      "NaN labels: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pcapml_fe\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Extract flow-level labels with flow_id\n",
    "flow_label_rows = []\n",
    "\n",
    "for tsample in pcapml_fe.sampler(str(PCAP_PATH)):\n",
    "    meta = str(tsample.metadata).strip()\n",
    "    parts = meta.split(\"_\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Unexpected metadata format for sample {tsample.sid}: {meta}\")\n",
    "    easy_lbl, med_lbl, hard_lbl = parts\n",
    "    \n",
    "    flow_label_rows.append({\n",
    "        \"flow_id\": str(tsample.sid),\n",
    "        \"easy_label\": easy_lbl,\n",
    "        \"medium_label\": med_lbl,\n",
    "        \"hard_label\": hard_lbl,\n",
    "    })\n",
    "\n",
    "flow_labels_df = pd.DataFrame(flow_label_rows)\n",
    "print(\"Flow labels shape:\", flow_labels_df.shape)\n",
    "\n",
    "# Step 2: Load all nprint CSVs\n",
    "dfs = []\n",
    "for csv_file in Path(\"nprint_outputs\").glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    stem = csv_file.stem  # e.g., \"12962054460325634226_audio_facebook\"\n",
    "    flow_id = stem.split(\"_\")[0]  # Take only the numeric ID\n",
    "    df[\"flow_id\"] = flow_id\n",
    "    dfs.append(df)\n",
    "\n",
    "nprint_df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"nPrint features shape:\", nprint_df.shape)\n",
    "\n",
    "# Step 3: Join features with labels\n",
    "final_df = nprint_df.merge(flow_labels_df, on=\"flow_id\", how=\"left\")\n",
    "print(\"Final shape:\", final_df.shape)\n",
    "print(final_df.head())\n",
    "\n",
    "# Check for NaN labels\n",
    "print(\"NaN labels:\", final_df[\"easy_label\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (7834, 1776)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Prepare features (run once)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, average_precision_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop non-feature columns\n",
    "X = final_df.drop(columns=[\"flow_id\", \"easy_label\", \"medium_label\", \"hard_label\", \"src_ip\"])\n",
    "\n",
    "# Handle any remaining NaN in features\n",
    "X = X.fillna(-1)\n",
    "\n",
    "print(\"Features shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Easy Label Classification\n",
    "\n",
    "y_easy = final_df[\"easy_label\"]\n",
    "\n",
    "# Encode labels\n",
    "le_easy = LabelEncoder()\n",
    "y_easy_encoded = le_easy.fit_transform(y_easy)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_easy_encoded, test_size=0.2, random_state=42, stratify=y_easy_encoded\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_easy = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_easy.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_easy = rf_easy.predict(X_test)\n",
    "y_proba_easy = rf_easy.predict_proba(X_test)\n",
    "\n",
    "# Metrics\n",
    "f1_easy = f1_score(y_test, y_pred_easy, average='weighted')\n",
    "print(f\"Easy Label - F1 Score (weighted): {f1_easy:.4f}\")\n",
    "\n",
    "# mAP (average precision per class, then mean)\n",
    "map_easy = np.mean([\n",
    "    average_precision_score((y_test == i).astype(int), y_proba_easy[:, i])\n",
    "    for i in range(len(le_easy.classes_))\n",
    "])\n",
    "print(f\"Easy Label - mAP: {map_easy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_easy, target_names=le_easy.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_easy = confusion_matrix(y_test, y_pred_easy)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_easy, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_easy.classes_, yticklabels=le_easy.classes_)\n",
    "plt.title('Easy Label - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Medium Label Classification\n",
    "\n",
    "y_medium = final_df[\"medium_label\"]\n",
    "\n",
    "# Encode labels\n",
    "le_medium = LabelEncoder()\n",
    "y_medium_encoded = le_medium.fit_transform(y_medium)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_medium_encoded, test_size=0.2, random_state=42, stratify=y_medium_encoded\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_medium = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_medium.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_medium = rf_medium.predict(X_test)\n",
    "y_proba_medium = rf_medium.predict_proba(X_test)\n",
    "\n",
    "# Metrics\n",
    "f1_medium = f1_score(y_test, y_pred_medium, average='weighted')\n",
    "print(f\"Medium Label - F1 Score (weighted): {f1_medium:.4f}\")\n",
    "\n",
    "# mAP\n",
    "map_medium = np.mean([\n",
    "    average_precision_score((y_test == i).astype(int), y_proba_medium[:, i])\n",
    "    for i in range(len(le_medium.classes_))\n",
    "])\n",
    "print(f\"Medium Label - mAP: {map_medium:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_medium, target_names=le_medium.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_medium = confusion_matrix(y_test, y_pred_medium)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_medium, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le_medium.classes_, yticklabels=le_medium.classes_)\n",
    "plt.title('Medium Label - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Hard Label Classification\n",
    "\n",
    "y_hard = final_df[\"hard_label\"]\n",
    "\n",
    "# Encode labels\n",
    "le_hard = LabelEncoder()\n",
    "y_hard_encoded = le_hard.fit_transform(y_hard)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_hard_encoded, test_size=0.2, random_state=42, stratify=y_hard_encoded\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_hard = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    n_estimators=100,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_hard = rf_hard.predict(X_test)\n",
    "y_proba_hard = rf_hard.predict_proba(X_test)\n",
    "\n",
    "# Metrics\n",
    "f1_hard = f1_score(y_test, y_pred_hard, average='weighted')\n",
    "print(f\"Hard Label - F1 Score (weighted): {f1_hard:.4f}\")\n",
    "\n",
    "# mAP\n",
    "map_hard = np.mean([\n",
    "    average_precision_score((y_test == i).astype(int), y_proba_hard[:, i])\n",
    "    for i in range(len(le_hard.classes_))\n",
    "])\n",
    "print(f\"Hard Label - mAP: {map_hard:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_hard, target_names=le_hard.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_hard = confusion_matrix(y_test, y_pred_hard)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_hard, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le_hard.classes_, yticklabels=le_hard.classes_)\n",
    "plt.title('Hard Label - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM on nPrint ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_svm_for_label_nprint(df, feature_cols, label_col, test_size=0.2, random_state=0):\n",
    "    \"\"\"\n",
    "    SVM experiment with an 80 / 20 train test split for nPrint\n",
    "\n",
    "    - 80 percent train, 20 percent test (stratified by label)\n",
    "    - Standardization + RBF SVM (C = 1.0, gamma = 'scale')\n",
    "    - Reports F1 (macro, weighted, micro) and mAP (macro) on the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Build X, y\n",
    "    X = df[feature_cols].values\n",
    "    y = df[label_col].values\n",
    "\n",
    "    # 2. 80 / 20 stratified train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    print(f\"[{label_col}] train size: {len(X_train)}, test size: {len(X_test)}\")\n",
    "\n",
    "    # 3. SVM pipeline  standardization + RBF SVM\n",
    "    svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svc\", SVC(\n",
    "            kernel=\"rbf\",\n",
    "            C=1.0,\n",
    "            gamma=\"scale\",\n",
    "            probability=True,  # needed for mAP\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 4. Train on full training set\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Predictions on test set\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "    # 6. F1 scores\n",
    "    f1_macro    = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_micro    = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "    # 7. mAP (macro, one versus rest)\n",
    "    classes = np.unique(y_train)  # classes seen in training\n",
    "    y_true_bin = label_binarize(y_test, classes=classes)\n",
    "    y_scores   = svm_clf.predict_proba(X_test)   # n_samples x n_classes\n",
    "    mAP_macro  = average_precision_score(y_true_bin, y_scores, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n[{label_col}] test F1 (macro):      {f1_macro:.4f}\")\n",
    "    print(f\"[{label_col}] test F1 (weighted):   {f1_weighted:.4f}\")\n",
    "    print(f\"[{label_col}] test F1 (micro):      {f1_micro:.4f}\")\n",
    "    print(f\"[{label_col}] test mAP (macro):     {mAP_macro:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report (test):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm_test = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": svm_clf,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"mAP_macro\": mAP_macro,\n",
    "        \"cm_test\": cm_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easy_label] train size: 6265, test size: 1567\n",
      "\n",
      "[easy_label] test F1 (macro):      0.6395\n",
      "[easy_label] test F1 (weighted):   0.8010\n",
      "[easy_label] test F1 (micro):      0.8296\n",
      "[easy_label] test mAP (macro):     0.7468\n",
      "\n",
      "Classification report (test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        audio       0.80      0.99      0.88       992\n",
      "         chat       0.91      0.29      0.43        35\n",
      "        email       0.98      0.81      0.89        54\n",
      "file-transfer       1.00      0.33      0.50       296\n",
      "          p2p       0.94      0.98      0.96        92\n",
      "          tor       0.00      0.00      0.00         1\n",
      "        video       0.90      0.74      0.81        97\n",
      "\n",
      "     accuracy                           0.83      1567\n",
      "    macro avg       0.79      0.59      0.64      1567\n",
      " weighted avg       0.86      0.83      0.80      1567\n",
      "\n",
      "[medium_label] train size: 6265, test size: 1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[medium_label] test F1 (macro):      0.5921\n",
      "[medium_label] test F1 (weighted):   0.6527\n",
      "[medium_label] test F1 (micro):      0.6541\n",
      "[medium_label] test mAP (macro):     0.8781\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         aim       0.00      0.00      0.00         3\n",
      "       email       0.96      0.89      0.92        54\n",
      "    facebook       0.55      0.53      0.54       348\n",
      "        ftps       0.98      0.83      0.90        48\n",
      "       gmail       1.00      0.25      0.40         4\n",
      "    hangouts       0.65      0.62      0.64       445\n",
      "         icq       0.00      0.00      0.00         2\n",
      "     netflix       0.00      0.00      0.00         2\n",
      "        sftp       1.00      0.95      0.98        22\n",
      "       skype       0.59      0.66      0.62       487\n",
      "     spotify       1.00      0.93      0.97        15\n",
      "     torrent       0.92      0.98      0.95        92\n",
      "       vimeo       0.84      0.80      0.82        20\n",
      "  voipbuster       0.59      0.73      0.65        22\n",
      "     youtube       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.65      1567\n",
      "   macro avg       0.67      0.57      0.59      1567\n",
      "weighted avg       0.66      0.65      0.65      1567\n",
      "\n",
      "[hard_label] train size: 6265, test size: 1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[hard_label] test F1 (macro):      0.5370\n",
      "[hard_label] test F1 (weighted):   0.5940\n",
      "[hard_label] test F1 (micro):      0.6324\n",
      "[hard_label] test mAP (macro):     0.7867\n",
      "\n",
      "Classification report (test):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      aim-chat       1.00      0.33      0.50         3\n",
      "         email       0.91      0.89      0.90        54\n",
      "facebook-audio       0.49      0.73      0.59       333\n",
      " facebook-chat       0.00      0.00      0.00         3\n",
      "facebook-video       1.00      0.18      0.31        11\n",
      "     ftps-down       1.00      0.20      0.33         5\n",
      "       ftps-up       1.00      1.00      1.00        43\n",
      "    gmail-chat       0.67      1.00      0.80         4\n",
      "hangouts-audio       0.55      0.82      0.65       411\n",
      " hangouts-chat       0.00      0.00      0.00         2\n",
      "hangouts-video       0.96      0.81      0.88        32\n",
      "      icq-chat       0.00      0.00      0.00         2\n",
      "       netflix       0.00      0.00      0.00         2\n",
      "       sftp-up       1.00      0.86      0.93        22\n",
      "   skype-audio       1.00      0.35      0.52       210\n",
      "    skype-chat       0.71      0.50      0.59        20\n",
      "    skype-file       0.95      0.09      0.16       227\n",
      "   skype-video       0.96      0.80      0.87        30\n",
      "       spotify       1.00      0.80      0.89        15\n",
      "   tor-youtube       0.00      0.00      0.00         1\n",
      "       torrent       0.91      0.97      0.94        92\n",
      "         vimeo       0.82      0.90      0.86        20\n",
      "    voipbuster       0.53      0.95      0.68        22\n",
      "       youtube       1.00      0.33      0.50         3\n",
      "\n",
      "      accuracy                           0.63      1567\n",
      "     macro avg       0.69      0.52      0.54      1567\n",
      "  weighted avg       0.74      0.63      0.59      1567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/connormcgraw/mambaforge/envs/ml-310-legacy/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "nprint_df = final_df.drop(columns=[\"flow_id\", \"src_ip\"])\n",
    "nprint_df.dropna(inplace=True)\n",
    "feature_cols = [\n",
    "    c for c in nprint_df.columns if c not in label_cols\n",
    "]\n",
    "\n",
    "nprint_res_easy = train_eval_svm_for_label_nprint(nprint_df, feature_cols, \"easy_label\")\n",
    "nprint_res_medium = train_eval_svm_for_label_nprint(nprint_df, feature_cols, \"medium_label\")\n",
    "nprint_res_hard = train_eval_svm_for_label_nprint(nprint_df, feature_cols, \"hard_label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
